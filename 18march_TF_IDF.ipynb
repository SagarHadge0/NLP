{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen â¦ thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency â¦ my entire team. I have to thank everyone from the very onset of my career â¦ To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65f268e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen â\\x80¦ thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency â\\x80¦ my entire team. I have to thank everyone from the very onset of my career â\\x80¦ To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99647060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b53bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc69570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to the Academy.',\n",
       " 'Thank you to all of you in this room.',\n",
       " 'I have to congratulate the other incredible nominees this year.',\n",
       " 'The Revenant was the product of the tireless efforts of an unbelievable cast and crew.',\n",
       " 'First off, to my brother in this endeavor, Mr. Tom Hardy.',\n",
       " 'Tom, your talent on screen can only be surpassed by your friendship off screen â\\x80¦ thank you for creating a transcendent cinematic experience.',\n",
       " 'Thank you to everybody at Fox and New Regency â\\x80¦ my entire team.',\n",
       " 'I have to thank everyone from the very onset of my career â\\x80¦ To my parents; none of this would be possible without you.',\n",
       " 'And to my friends, I love you dearly; you know who you are.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac63a95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba668d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9898158",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datasets)):\n",
    "    datasets[i] = datasets[i].lower()\n",
    "    datasets[i] = re.sub(r'\\W',' ',datasets[i])\n",
    "    datasets[i] = re.sub(r'\\s+',' ',datasets[i])\n",
    "    datasets[i] = re.sub(r's+$', '',datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f36a17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so very much ',\n",
       " 'thank you to the academy ',\n",
       " 'thank you to all of you in this room ',\n",
       " 'i have to congratulate the other incredible nominees this year ',\n",
       " 'the revenant was the product of the tireless efforts of an unbelievable cast and crew ',\n",
       " 'first off to my brother in this endeavor mr tom hardy ',\n",
       " 'tom your talent on screen can only be surpassed by your friendship off screen â thank you for creating a transcendent cinematic experience ',\n",
       " 'thank you to everybody at fox and new regency â my entire team ',\n",
       " 'i have to thank everyone from the very onset of my career â to my parents none of this would be possible without you ',\n",
       " 'and to my friends i love you dearly you know who you are ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e37b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for data in datasets:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "031f61cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 6,\n",
       " 'you': 10,\n",
       " 'all': 2,\n",
       " 'so': 1,\n",
       " 'very': 2,\n",
       " 'much': 1,\n",
       " 'to': 8,\n",
       " 'the': 6,\n",
       " 'academy': 1,\n",
       " 'of': 5,\n",
       " 'in': 2,\n",
       " 'this': 4,\n",
       " 'room': 1,\n",
       " 'i': 3,\n",
       " 'have': 2,\n",
       " 'congratulate': 1,\n",
       " 'other': 1,\n",
       " 'incredible': 1,\n",
       " 'nominees': 1,\n",
       " 'year': 1,\n",
       " 'revenant': 1,\n",
       " 'was': 1,\n",
       " 'product': 1,\n",
       " 'tireless': 1,\n",
       " 'efforts': 1,\n",
       " 'an': 1,\n",
       " 'unbelievable': 1,\n",
       " 'cast': 1,\n",
       " 'and': 3,\n",
       " 'crew': 1,\n",
       " 'first': 1,\n",
       " 'off': 2,\n",
       " 'my': 5,\n",
       " 'brother': 1,\n",
       " 'endeavor': 1,\n",
       " 'mr': 1,\n",
       " 'tom': 2,\n",
       " 'hardy': 1,\n",
       " 'your': 2,\n",
       " 'talent': 1,\n",
       " 'on': 1,\n",
       " 'screen': 2,\n",
       " 'can': 1,\n",
       " 'only': 1,\n",
       " 'be': 2,\n",
       " 'surpassed': 1,\n",
       " 'by': 1,\n",
       " 'friendship': 1,\n",
       " 'â': 3,\n",
       " 'for': 1,\n",
       " 'creating': 1,\n",
       " 'a': 1,\n",
       " 'transcendent': 1,\n",
       " 'cinematic': 1,\n",
       " 'experience': 1,\n",
       " 'everybody': 1,\n",
       " 'at': 1,\n",
       " 'fox': 1,\n",
       " 'new': 1,\n",
       " 'regency': 1,\n",
       " 'entire': 1,\n",
       " 'team': 1,\n",
       " 'everyone': 1,\n",
       " 'from': 1,\n",
       " 'onset': 1,\n",
       " 'career': 1,\n",
       " 'parents': 1,\n",
       " 'none': 1,\n",
       " 'would': 1,\n",
       " 'possible': 1,\n",
       " 'without': 1,\n",
       " 'friends': 1,\n",
       " 'love': 1,\n",
       " 'dearly': 1,\n",
       " 'know': 1,\n",
       " 'who': 1,\n",
       " 'are': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3164dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "463d8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = heapq.nlargest(15,word2count,key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a31a29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'the',\n",
       " 'of',\n",
       " 'my',\n",
       " 'this',\n",
       " 'i',\n",
       " 'and',\n",
       " 'â',\n",
       " 'all',\n",
       " 'very',\n",
       " 'in',\n",
       " 'have',\n",
       " 'off']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba874d",
   "metadata": {},
   "source": [
    "# IDF\n",
    "IDF is typically used to boost the scores of words that are unique to a document with the hope that you surface high information words that characterize your document and suppress words that don't carry much weight in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6465cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word_idfs ={}\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in datasets:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log((len(datasets)/(doc_count)+1))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9384e578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0.8873031950009029,\n",
       " 'to': 0.8873031950009029,\n",
       " 'thank': 0.9808292530117263,\n",
       " 'the': 1.252762968495368,\n",
       " 'of': 1.4663370687934272,\n",
       " 'my': 1.252762968495368,\n",
       " 'this': 1.252762968495368,\n",
       " 'i': 1.4663370687934272,\n",
       " 'and': 1.4663370687934272,\n",
       " 'â': 1.4663370687934272,\n",
       " 'all': 1.791759469228055,\n",
       " 'very': 1.791759469228055,\n",
       " 'in': 1.791759469228055,\n",
       " 'have': 1.791759469228055,\n",
       " 'off': 1.791759469228055}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c6c47",
   "metadata": {},
   "source": [
    "# TF\n",
    "Term Frequency: TF of a term or word is the number of times the term appears in a document compared to the total number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a8c2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix ={}\n",
    "for word in freq_words:\n",
    "    doc_tf =[]\n",
    "    for data in datasets:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w == word:\n",
    "                frequency += 1\n",
    "         \n",
    "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] =doc_tf    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135a1d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': [0.16666666666666666,\n",
       "  0.2,\n",
       "  0.2222222222222222,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.043478260869565216,\n",
       "  0.07692307692307693,\n",
       "  0.041666666666666664,\n",
       "  0.23076923076923078],\n",
       " 'to': [0.0,\n",
       "  0.2,\n",
       "  0.1111111111111111,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.0,\n",
       "  0.07692307692307693,\n",
       "  0.08333333333333333,\n",
       "  0.07692307692307693],\n",
       " 'thank': [0.16666666666666666,\n",
       "  0.2,\n",
       "  0.1111111111111111,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.043478260869565216,\n",
       "  0.07692307692307693,\n",
       "  0.041666666666666664,\n",
       "  0.0],\n",
       " 'the': [0.0, 0.2, 0.0, 0.1, 0.2, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0],\n",
       " 'of': [0.0,\n",
       "  0.0,\n",
       "  0.1111111111111111,\n",
       "  0.0,\n",
       "  0.13333333333333333,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08333333333333333,\n",
       "  0.0],\n",
       " 'my': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.0,\n",
       "  0.07692307692307693,\n",
       "  0.08333333333333333,\n",
       "  0.07692307692307693],\n",
       " 'this': [0.0,\n",
       "  0.0,\n",
       "  0.1111111111111111,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.041666666666666664,\n",
       "  0.0],\n",
       " 'i': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.041666666666666664,\n",
       "  0.07692307692307693],\n",
       " 'and': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.06666666666666667,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07692307692307693,\n",
       "  0.0,\n",
       "  0.07692307692307693],\n",
       " 'â': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.043478260869565216,\n",
       "  0.07692307692307693,\n",
       "  0.041666666666666664,\n",
       "  0.0],\n",
       " 'all': [0.16666666666666666,\n",
       "  0.0,\n",
       "  0.1111111111111111,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'very': [0.16666666666666666,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.041666666666666664,\n",
       "  0.0],\n",
       " 'in': [0.0,\n",
       "  0.0,\n",
       "  0.1111111111111111,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'have': [0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0],\n",
       " 'off': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.043478260869565216,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a69cdc",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84379086",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value *word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a3f5424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16288722447527773,\n",
       " 0.07790258561861109,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
